{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOAH Downloads Scraper\n",
    "### About\n",
    "This Notebook is used for scraping/downloading the datasets in the Project NOAH [website](http://noah.up.edu.ph/) and serves as a basic exercise in scraping using Beautiful Soup and Python 3.\n",
    "\n",
    "### RUNNING THE TOOL\n",
    "_**REQUIREMENTS**_\n",
    "* Python 3.6.1\n",
    "* jupyter\n",
    "* Beautiful Soup\n",
    "\n",
    "They are also found in the __requirements.txt__ file.  \n",
    "You can install these requirements using __pip__ (__sudo pip install -r requirements.txt__ _or_ __pip install -r requirements.txt__)  \n",
    "\n",
    "_**PROCEDURE**_\n",
    "1. Choose the URLs to be scraped in the URL list.\n",
    "2. Add the proxy in PROXY (if any).\n",
    "3. Select the root directory to save the scraped files to (ROOT_SAVEDIR).\n",
    "4. Add a QUERY list to limit the download (i.e. [\"Albay\", \"Abra\"]; only files containing strings matching any of the elements in QUERY will be downloaded).\n",
    "5. Run All.\n",
    "\n",
    "### LICENSE  \n",
    "_Copyright (C) 2017 Ben Hur S. Pintor (bhs.pintor@gmail.com)_ [[website](https://benhur07b.github.io)]\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import shutil\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "NOAH_URL = \"http://noah.up.edu.ph/downloads\"\n",
    "LANDSLIDE_RAS_URL = \"LANDSLIDE/RASTER/HAZARDS\"\n",
    "LANDSLIDE_SHP_URL = \"LANDSLIDE/SHAPEFILES/HAZARDS\"\n",
    "STORMSURGE_RAS_SSA1_URL = \"STORMSURGE/RASTER/SSA1\"\n",
    "STORMSURGE_RAS_SSA2_URL = \"STORMSURGE/RASTER/SSA2\"\n",
    "STORMSURGE_RAS_SSA3_URL = \"STORMSURGE/RASTER/SSA3\"\n",
    "STORMSURGE_RAS_SSA4_URL = \"STORMSURGE/RASTER/SSA4\"\n",
    "STORMSURGE_SHP_SSA1_URL = \"STORMSURGE/SHAPEFILES/SSA1\"\n",
    "STORMSURGE_SHP_SSA2_URL = \"STORMSURGE/SHAPEFILES/SSA2\"\n",
    "STORMSURGE_SHP_SSA3_URL = \"STORMSURGE/SHAPEFILES/SSA3\"\n",
    "STORMSURGE_SHP_SSA4_URL = \"STORMSURGE/SHAPEFILES/SSA4\"\n",
    "\n",
    "# add the URLs to scrape here\n",
    "URLS = [STORMSURGE_SHP_SSA1_URL,\n",
    "        STORMSURGE_SHP_SSA2_URL,\n",
    "        STORMSURGE_SHP_SSA3_URL,\n",
    "        STORMSURGE_SHP_SSA4_URL]\n",
    "\n",
    "PROXY = \"\"  # insert proxy here\n",
    "\n",
    "PROXIES = {\"http\": PROXY}\n",
    "\n",
    "ROOT_SAVEDIR = \"\"  # add directory to save scraped files here (absolute path)\n",
    "\n",
    "QUERY = []  # add query items (strings to limit downloads) here, leave empty to download ALL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_links(r, query):\n",
    "    \"\"\"Returns a list of links in a page provided for by a requests object r.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    r -- the requests object\n",
    "    \"\"\"\n",
    "    \n",
    "    links = list()\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    if len(query) > 0:\n",
    "        return [l for l in links if any(q in l for q in query)]  # return only the links that match any of the query items\n",
    "                                                                 # for every l in links, checks if l contains any q in query\n",
    "    else:\n",
    "        return links[1:]\n",
    "\n",
    "    \n",
    "def save_to_file(url, name, proxies=None):\n",
    "    \"\"\"Saves the sensor measurements in the url into a .csv file.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- the url of the sensor measurement\n",
    "    name -- the name of the file to save\n",
    "    proxies -- the proxy settings, if any (default = None)\n",
    "    \"\"\"\n",
    "    \n",
    "    if proxies:\n",
    "        r = requests.get(url, stream=True, proxies=proxies)\n",
    "    else:\n",
    "        r = requests.get(url, stream=True)\n",
    "        \n",
    "    with open(name, 'wb') as outfile:\n",
    "        shutil.copyfileobj(r.raw, outfile)\n",
    "    \n",
    "    del r\n",
    "    print(\"{} saved.\".format(name))\n",
    "\n",
    "\n",
    "for url in URLS:\n",
    "    \n",
    "    s_url = \"{}/{}/\".format(NOAH_URL, url)\n",
    "    \n",
    "    SAVEDIR = \"{}/{}\".format(ROOT_SAVEDIR, url)\n",
    "    \n",
    "    if SAVEDIR:\n",
    "        if not os.path.exists(SAVEDIR):\n",
    "            os.makedirs(SAVEDIR)\n",
    "\n",
    "        os.chdir(SAVEDIR)\n",
    "    \n",
    "    r = requests.get(s_url, proxies=PROXIES)\n",
    "    links = get_links(r, QUERY)\n",
    "    for link in links:\n",
    "        if not os.path.exists(link):\n",
    "            save_to_file(\"{}{}\".format(s_url, link), link, PROXIES)\n",
    "        else:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
